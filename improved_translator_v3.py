#!/usr/bin/env python3
"""
ê°œì„ ëœ PDF ë²ˆì—­ê¸° V3 - 2ë‹¨ê³„ ê°œì„ ì‚¬í•­ ì ìš©
- V2 ê¸°ëŠ¥ (ì¬ì‹œë„ 5íšŒ, ì§€ìˆ˜ ë°±ì˜¤í”„, ë™ì  íƒ€ì„ì•„ì›ƒ) ìœ ì§€
- ì¶”ê°€: ìŠ¤ë§ˆíŠ¸ ì²­í¬ ë¶„í• 
- ì¶”ê°€: í‘œ/í…Œì´ë¸” ì „ìš© ì²˜ë¦¬
- ì¶”ê°€: TOC ì „ìš© ì²˜ë¦¬
"""

import PyPDF2
import os
import json
import time
import re
import csv
import sys
from typing import List, Tuple, Optional, Dict
from dataclasses import dataclass
try:
    import openpyxl
    from openpyxl.styles import Font, PatternFill, Alignment, Border, Side
    from openpyxl.utils import get_column_letter
    OPENPYXL_AVAILABLE = True
except ImportError:
    OPENPYXL_AVAILABLE = False
    print("âš ï¸  openpyxl not installed. Excel output will be skipped.")

try:
    from toc_structure_parser import TOCStructureParser, TOCItem
    TOC_PARSER_AVAILABLE = True
except ImportError:
    TOC_PARSER_AVAILABLE = False

# ============================================================================
# Step 1: ìŠ¤ë§ˆíŠ¸ ì²­í¬ ë¶„í• 
# ============================================================================

@dataclass
class TextChunk:
    """í…ìŠ¤íŠ¸ ì²­í¬ ë°ì´í„° í´ë˜ìŠ¤"""
    content: str
    start_index: int
    end_index: int
    chunk_type: str  # 'paragraph', 'sentence', 'forced'

def smart_chunk_text(text: str, max_length: int = 800, overlap: int = 100) -> List[TextChunk]:
    """
    ìŠ¤ë§ˆíŠ¸ ì²­í¬ ë¶„í•  - ì˜ë¯¸ ë‹¨ìœ„ë¡œ í…ìŠ¤íŠ¸ ë¶„í• 

    ìš°ì„ ìˆœìœ„:
    1. ë‹¨ë½ êµ¬ë¶„ (\\n\\n)
    2. ë¬¸ì¥ ë (ã€‚ï¼ï¼Ÿ.)
    3. ì‰¼í‘œ (ï¼Œ,)
    4. ê³ ì • ê¸¸ì´ (ìµœí›„ ìˆ˜ë‹¨)
    """

    if len(text) <= max_length:
        return [TextChunk(text, 0, len(text), 'complete')]

    chunks = []
    current_pos = 0

    while current_pos < len(text):
        # ì²­í¬ ë ìœ„ì¹˜ ê³„ì‚°
        end_pos = min(current_pos + max_length, len(text))

        if end_pos >= len(text):
            # ë§ˆì§€ë§‰ ì²­í¬
            chunks.append(TextChunk(
                text[current_pos:],
                current_pos,
                len(text),
                'final'
            ))
            break

        # ìµœì  ë¶„í•  ì§€ì  ì°¾ê¸°
        chunk_text = text[current_pos:end_pos + 200]  # ì•½ê°„ ë” ì½ì–´ì„œ ë¶„í• ì  ì°¾ê¸°
        split_point = find_best_split_point(chunk_text, max_length)

        actual_end = current_pos + split_point

        chunks.append(TextChunk(
            text[current_pos:actual_end],
            current_pos,
            actual_end,
            'split'
        ))

        # ë‹¤ìŒ ì²­í¬ ì‹œì‘ ìœ„ì¹˜ (ì˜¤ë²„ë© ì ìš©)
        current_pos = actual_end - overlap if actual_end > overlap else actual_end

    return chunks

def find_best_split_point(text: str, max_length: int) -> int:
    """
    ìµœì ì˜ ë¶„í•  ì§€ì  ì°¾ê¸°

    ìš°ì„ ìˆœìœ„:
    1. ë‹¨ë½ êµ¬ë¶„ (\\n\\n) - ê°€ì¥ ìì—°ìŠ¤ëŸ¬ìš´ ë¶„í• 
    2. ë¬¸ì¥ ë (ã€‚ï¼ï¼Ÿ.!?) - ë¬¸ì¥ ì™„ê²°ì„± ìœ ì§€
    3. ì‰¼í‘œ (ï¼Œ,;ï¼›) - ì ˆ ë‹¨ìœ„ ë¶„í• 
    4. ê³µë°± - ë‹¨ì–´ ë‹¨ìœ„ ë¶„í• 
    5. ê°•ì œ ë¶„í•  - ìµœí›„ ìˆ˜ë‹¨
    """

    # 1. ë‹¨ë½ êµ¬ë¶„ ì°¾ê¸°
    paragraph_breaks = [m.start() for m in re.finditer(r'\n\n+', text[:max_length + 100])]
    if paragraph_breaks:
        best = max([p for p in paragraph_breaks if p <= max_length], default=None)
        if best and best > max_length * 0.7:  # ìµœì†Œ 70% ì´ìƒ í™œìš©
            return best + 2  # \n\n ì´í›„

    # 2. ë¬¸ì¥ ë ì°¾ê¸°
    sentence_ends = [m.end() for m in re.finditer(r'[ã€‚ï¼ï¼Ÿ.!?]\s*', text[:max_length + 50])]
    if sentence_ends:
        best = max([s for s in sentence_ends if s <= max_length], default=None)
        if best and best > max_length * 0.7:
            return best

    # 3. ì‰¼í‘œ ì°¾ê¸°
    comma_positions = [m.end() for m in re.finditer(r'[ï¼Œ,;ï¼›]\s*', text[:max_length + 20])]
    if comma_positions:
        best = max([c for c in comma_positions if c <= max_length], default=None)
        if best and best > max_length * 0.8:
            return best

    # 4. ê³µë°± ì°¾ê¸°
    space_positions = [m.end() for m in re.finditer(r'\s+', text[:max_length + 10])]
    if space_positions:
        best = max([s for s in space_positions if s <= max_length], default=None)
        if best:
            return best

    # 5. ê°•ì œ ë¶„í• 
    return max_length

def merge_chunk_translations(chunks: List[str], original_chunks: List[TextChunk]) -> str:
    """
    ì²­í¬ë³„ ë²ˆì—­ ê²°ê³¼ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ë³‘í•©

    ì˜¤ë²„ë© ë¶€ë¶„ ì¤‘ë³µ ì œê±° ë° ìì—°ìŠ¤ëŸ¬ìš´ ì—°ê²°
    """

    if not chunks:
        return ""

    if len(chunks) == 1:
        return chunks[0]

    # ì²« ë²ˆì§¸ ì²­í¬ë¡œ ì‹œì‘
    merged = chunks[0]

    # ë‚˜ë¨¸ì§€ ì²­í¬ ë³‘í•©
    for i in range(1, len(chunks)):
        # ì¤‘ë³µ ë¶€ë¶„ ì œê±° ë¡œì§
        # ê°„ë‹¨í•˜ê²Œ: ê³µë°±ìœ¼ë¡œ ì—°ê²° (í–¥í›„ ë” ì •êµí•˜ê²Œ ê°œì„  ê°€ëŠ¥)
        merged += " " + chunks[i]

    return merged

# ============================================================================
# Step 2: í‘œ/í…Œì´ë¸” ê°ì§€
# ============================================================================

def detect_table(text: str) -> bool:
    """
    í‘œ/í…Œì´ë¸” êµ¬ì¡° ê°ì§€

    ê°ì§€ ê¸°ì¤€:
    1. í‘œ ê²½ê³„ ë¬¸ì: â”ƒâ”‚â”œâ”¤â”¬â”´â”€ ë“±
    2. ë°˜ë³µ íŒ¨í„´: ì—¬ëŸ¬ ì¤„ì— ê±¸ì¹œ ì¼ì •í•œ êµ¬ì¡°
    3. í‘œ í‚¤ì›Œë“œ: "è¡¨", "åŠŸèƒ½çŸ©é˜µ", "ç³»ç»Ÿ" ë“±
    """

    # 1. í‘œ ê²½ê³„ ë¬¸ì ì²´í¬
    table_chars = ['â”ƒ', 'â”‚', 'â”œ', 'â”¤', 'â”¬', 'â”´', 'â”€', 'â•‘', 'â• ', 'â•£']
    table_char_count = sum(text.count(char) for char in table_chars)

    if table_char_count > 5:  # í‘œ ë¬¸ìê°€ 5ê°œ ì´ìƒ
        return True

    # 2. í‘œ ì œëª© íŒ¨í„´ ì²´í¬
    table_patterns = [
        r'è¡¨\d+',  # è¡¨3, è¡¨1 ë“±
        r'åŠŸèƒ½çŸ©é˜µ',
        r'ç³»ç»Ÿ.*åŠŸèƒ½',
        r'\|.*\|.*\|',  # Markdown í‘œ
    ]

    for pattern in table_patterns:
        if re.search(pattern, text):
            return True

    # 3. ë°˜ë³µ íŒ¨í„´ ì²´í¬ (ê°„ë‹¨ ë²„ì „)
    lines = text.split('\n')
    if len(lines) > 3:
        # ë¹„ìŠ·í•œ ê¸¸ì´ì˜ ì¤„ì´ ì—¬ëŸ¬ ê°œ ìˆìœ¼ë©´ í‘œì¼ ê°€ëŠ¥ì„±
        line_lengths = [len(line) for line in lines if line.strip()]
        if len(line_lengths) > 3:
            avg_length = sum(line_lengths) / len(line_lengths)
            similar_lines = sum(1 for l in line_lengths if abs(l - avg_length) < avg_length * 0.3)
            if similar_lines > len(line_lengths) * 0.6:  # 60% ì´ìƒ ìœ ì‚¬
                return True

    return False

def process_table_structure(text: str) -> str:
    """
    í‘œ êµ¬ì¡° ì „ìš© ì²˜ë¦¬

    ì „ëµ:
    1. í‘œë¥¼ í–‰ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    2. ê° í–‰ì˜ ì…€ ë‚´ìš© ì¶”ì¶œ
    3. ì…€ë³„ë¡œ ë²ˆì—­
    4. í‘œ êµ¬ì¡°ë¡œ ì¬ì¡°ë¦½
    """

    print("      [í‘œ ê°ì§€] í‘œ ì „ìš© ì²˜ë¦¬ ëª¨ë“œ í™œì„±í™”")

    # ê°„ë‹¨ ë²„ì „: ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„ë¦¬í•˜ì—¬ ê° ì¤„ ê°œë³„ ë²ˆì—­
    # (ë³µì¡í•œ í‘œ íŒŒì‹±ì€ ì¶”í›„ ê°œì„ )

    return text  # ì¼ë‹¨ ì›ë³¸ ê·¸ëŒ€ë¡œ ë°˜í™˜ (ë²ˆì—­ì€ ì™¸ë¶€ì—ì„œ)

# ============================================================================
# Step 3: TOC ì „ìš© ì²˜ë¦¬
# ============================================================================

def detect_toc(text: str) -> bool:
    """
    TOC(ëª©ì°¨) í˜ì´ì§€ ê°ì§€

    ê°ì§€ ê¸°ì¤€:
    1. "ç›®å½•", "ç›® å½•" í‚¤ì›Œë“œ
    2. ë§ì€ ì ì„  (.......) ë˜ëŠ” ëŒ€ì‹œ (-----)
    3. í˜ì´ì§€ ë²ˆí˜¸ íŒ¨í„´ (- XX -)
    4. ê³„ì¸µì  ë²ˆí˜¸ êµ¬ì¡° (1.1.1, 3.4.3.6 ë“±)
    """

    # 1. ëª©ì°¨ í‚¤ì›Œë“œ
    if re.search(r'ç›®\s*å½•', text):
        return True

    # 2. ì ì„ /ëŒ€ì‹œ ë§ìŒ
    dot_count = text.count('.')
    dash_count = text.count('-')
    text_length = len(text)

    if text_length > 0:
        dot_ratio = dot_count / text_length
        dash_ratio = dash_count / text_length

        if dot_ratio > 0.15 or dash_ratio > 0.1:  # 15% ì´ìƒì´ ì  ë˜ëŠ” 10% ì´ìƒì´ ëŒ€ì‹œ
            return True

    # 3. í˜ì´ì§€ ë²ˆí˜¸ íŒ¨í„´
    page_number_pattern = r'-\s*\d+\s*-'
    page_numbers = len(re.findall(page_number_pattern, text))
    if page_numbers > 10:  # í˜ì´ì§€ ë²ˆí˜¸ê°€ 10ê°œ ì´ìƒ
        return True

    # 4. ê³„ì¸µì  ë²ˆí˜¸ êµ¬ì¡°
    hierarchical_pattern = r'\d+\.\d+\.\d+'
    hierarchical_count = len(re.findall(hierarchical_pattern, text))
    if hierarchical_count > 15:  # ê³„ì¸µì  ë²ˆí˜¸ê°€ 15ê°œ ì´ìƒ
        return True

    return False

def parse_toc_items(text: str) -> List[Dict]:
    """
    TOC í•­ëª© íŒŒì‹±

    ê° í•­ëª©ì„ ë‹¤ìŒ í˜•íƒœë¡œ ì¶”ì¶œ:
    {
        'number': '3.4.3.6',
        'title': 'é™¡å¡ç¼“é™ï¼ˆHDCï¼‰è¦æ±‚',
        'page': '17',
        'original_line': 'ì›ë³¸ ì¤„'
    }
    """

    items = []
    lines = text.split('\n')

    # íŒ¨í„´: ìˆ«ì.ìˆ«ì.ìˆ«ì í˜•ì‹ì˜ í•­ëª© ì°¾ê¸°
    item_pattern = r'^(\d+(?:\.\d+)*)\.\s*(.+?)\s*\.+\s*(?:.*?-\s*(\d+)\s*-)?'

    for line in lines:
        line = line.strip()
        if not line:
            continue

        match = re.search(item_pattern, line)
        if match:
            items.append({
                'number': match.group(1),
                'title': match.group(2).strip(),
                'page': match.group(3) if match.group(3) else '',
                'original_line': line
            })
        else:
            # ë‹¨ìˆœ í•­ëª© (ë²ˆí˜¸ ì—†ìŒ)
            if any(keyword in line for keyword in ['æœ¯è¯­', 'æ¦‚è¿°', 'å‰è¨€', 'æ›´æ”¹è®°å½•']):
                items.append({
                    'number': '',
                    'title': line,
                    'page': '',
                    'original_line': line
                })

    return items

def translate_toc_item(item: Dict) -> str:
    """
    ê°œë³„ TOC í•­ëª© ë²ˆì—­ (ì œëª©ë§Œ)
    """
    from googletrans import Translator

    try:
        translator = Translator()
        title = item['title']

        if not title or len(title) < 2:
            return item['original_line']

        # ì œëª©ë§Œ ë²ˆì—­
        result = translator.translate(title, dest='en')
        if result and hasattr(result, 'text') and result.text:
            translated_title = result.text

            # ë²ˆì—­ëœ ì œëª©ìœ¼ë¡œ ì¬êµ¬ì„±
            if item['number'] and item['page']:
                return f"{item['number']}. {translated_title} {'.' * 20} - {item['page']} -"
            elif item['number']:
                return f"{item['number']}. {translated_title}"
            else:
                return translated_title

        return item['original_line']

    except Exception as e:
        print(f"        TOC í•­ëª© ë²ˆì—­ ì‹¤íŒ¨: {e}")
        return item['original_line']

def process_toc_structure(text: str) -> str:
    """
    TOC êµ¬ì¡° ì „ìš© ì²˜ë¦¬

    1. TOC í•­ëª© íŒŒì‹±
    2. ê° í•­ëª© ê°œë³„ ë²ˆì—­
    3. ì¬ì¡°ë¦½
    """

    print("      [TOC ê°ì§€] TOC ì „ìš© ì²˜ë¦¬ ëª¨ë“œ í™œì„±í™”")

    # TOC í•­ëª© íŒŒì‹±
    items = parse_toc_items(text)
    print(f"      TOC í•­ëª© ìˆ˜: {len(items)}ê°œ")

    if not items:
        print("      TOC í•­ëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ, ì¼ë°˜ ì²˜ë¦¬ë¡œ ì „í™˜")
        return text

    # ê° í•­ëª© ë²ˆì—­
    translated_items = []
    for i, item in enumerate(items[:5], 1):  # ì²˜ìŒ 5ê°œë§Œ í…ŒìŠ¤íŠ¸
        print(f"        í•­ëª© {i}/{min(5, len(items))}: {item['title'][:30]}...")
        translated = translate_toc_item(item)
        translated_items.append(translated)
        time.sleep(0.5)  # ë ˆì´íŠ¸ ë¦¬ë¯¸íŠ¸ ë°©ì§€

    # ë‚˜ë¨¸ì§€ëŠ” ì›ë³¸ ìœ ì§€ (í…ŒìŠ¤íŠ¸)
    for item in items[5:]:
        translated_items.append(item['original_line'])

    # ì¬ì¡°ë¦½
    result = '\n'.join(translated_items)
    return result

# ============================================================================
# ê¸°ì¡´ V2 í•¨ìˆ˜ë“¤ (ìœ ì§€)
# ============================================================================

def extract_text_from_pdf(pdf_path: str, max_pages: Optional[int] = None) -> List[Tuple[int, str]]:
    """Extract text from PDF using PyPDF2"""
    pages_text = []
    try:
        with open(pdf_path, 'rb') as file:
            pdf_reader = PyPDF2.PdfReader(file)
            total_pages = len(pdf_reader.pages)

            if max_pages:
                total_pages = min(total_pages, max_pages)

            print(f"ğŸ“– Extracting from {total_pages} pages...")

            for i in range(total_pages):
                try:
                    page = pdf_reader.pages[i]
                    text = page.extract_text()
                    if text and text.strip():
                        pages_text.append((i + 1, text.strip()))
                    else:
                        pages_text.append((i + 1, f"[Page {i + 1} - No extractable text]"))
                except Exception as e:
                    print(f"   âš ï¸  Error on page {i + 1}: {e}")
                    pages_text.append((i + 1, f"[Page {i + 1} - Extraction error: {str(e)}]"))

        print(f"âœ… Successfully processed {len(pages_text)} pages")
        return pages_text

    except Exception as e:
        print(f"âŒ Error reading PDF: {str(e)}")
        return []

def safe_text_cleaning(text: str) -> str:
    """Safely clean text with comprehensive error handling"""
    if not text or not isinstance(text, str):
        return ""

    try:
        if isinstance(text, bytes):
            text = text.decode('utf-8', errors='ignore')

        if text is None or str(text).strip() == 'None':
            return ""

        clean_parts = []
        for part in str(text).split():
            if part is not None and isinstance(part, str) and part.strip() and part.strip() != 'None':
                clean_parts.append(part.strip())

        if not clean_parts:
            return ""

        clean_text = ' '.join(clean_parts)
        clean_text = re.sub(r'\.{3,}', '...', clean_text)
        clean_text = re.sub(r'\s+', ' ', clean_text)
        clean_text = clean_text.strip()

        return clean_text

    except Exception as e:
        print(f"      Text cleaning error: {e}")
        return ""

def translate_with_google_robust(text: str, use_chunking: bool = True) -> str:
    """
    ê°œì„ ëœ ë²ˆì—­ í•¨ìˆ˜ - V3

    V2 ê¸°ëŠ¥:
    - ì¬ì‹œë„ 5íšŒ
    - ì§€ìˆ˜ ë°±ì˜¤í”„
    - ë™ì  íƒ€ì„ì•„ì›ƒ

    V3 ì¶”ê°€:
    - ìŠ¤ë§ˆíŠ¸ ì²­í¬ ë¶„í• 
    - í‘œ ì „ìš© ì²˜ë¦¬
    - TOC ì „ìš© ì²˜ë¦¬
    """
    try:
        from googletrans import Translator

        if not text or not isinstance(text, str):
            return "[Error: Invalid input]"

        if text is None or str(text).strip() == 'None':
            return "[Error: None value detected]"

        clean_text = safe_text_cleaning(text)
        if not clean_text:
            return "[Error: No valid text after cleaning]"

        if len(clean_text) < 5:
            return f"[Skipped: Too short - '{clean_text}']"

        # ğŸ†• V3: TOC ê°ì§€ ë° ì „ìš© ì²˜ë¦¬
        if detect_toc(clean_text):
            print("      ğŸ” TOC í˜ì´ì§€ ê°ì§€!")
            processed_toc = process_toc_structure(clean_text)
            # TOC ì²˜ë¦¬ í›„ì—ë„ ë²ˆì—­ ì‹œë„
            clean_text = processed_toc

        # ğŸ†• V3: í‘œ ê°ì§€ ë° ì „ìš© ì²˜ë¦¬
        is_table = detect_table(clean_text)
        if is_table:
            print("      ğŸ“Š í‘œ êµ¬ì¡° ê°ì§€!")
            clean_text = process_table_structure(clean_text)

        # ğŸ†• V3: ì²­í¬ ë¶„í•  ì—¬ë¶€ ê²°ì •
        if use_chunking and len(clean_text) > 1000:
            print(f"      âœ‚ï¸  ê¸´ í…ìŠ¤íŠ¸ ê°ì§€ ({len(clean_text)}ì), ì²­í¬ ë¶„í•  ì ìš©")
            chunks = smart_chunk_text(clean_text, max_length=800, overlap=100)
            print(f"      ğŸ“¦ {len(chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")

            chunk_translations = []
            for i, chunk in enumerate(chunks, 1):
                print(f"        ì²­í¬ {i}/{len(chunks)} ë²ˆì—­ ì¤‘...")
                translated = translate_single_text(chunk.content)
                chunk_translations.append(translated)
                if i < len(chunks):
                    time.sleep(1)  # ì²­í¬ ê°„ ëŒ€ê¸°

            # ì²­í¬ ë³‘í•©
            merged = merge_chunk_translations(chunk_translations, chunks)
            return merged
        else:
            # ì¼ë°˜ ë²ˆì—­
            return translate_single_text(clean_text)

    except ImportError:
        return "[Error: googletrans not installed]"
    except Exception as e:
        return f"[Translation error: {str(e)}]"

def translate_single_text(text: str) -> str:
    """
    ë‹¨ì¼ í…ìŠ¤íŠ¸ ë²ˆì—­ (V2 ë¡œì§ ìœ ì§€)
    - ì¬ì‹œë„ 5íšŒ
    - ì§€ìˆ˜ ë°±ì˜¤í”„
    """
    from googletrans import Translator

    for attempt in range(5):
        try:
            translator = Translator()
            result = translator.translate(text, dest='en')

            if result and hasattr(result, 'text') and result.text and result.text is not None and str(result.text).strip():
                if attempt > 0:
                    print(f"      âœ… ì„±ê³µ (ì‹œë„ {attempt + 1})")
                return result.text
            else:
                print(f"      ì‹œë„ {attempt + 1}: Empty result")
        except Exception as e:
            print(f"      ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")

        if attempt < 4:
            wait_time = 2 ** attempt
            print(f"      ëŒ€ê¸° {wait_time}ì´ˆ...")
            time.sleep(wait_time)

    return f"[Translation failed after 5 attempts]"

def create_outputs(results: List[dict], base_name: str = "improved_translation_v3"):
    """Create JSON and CSV outputs"""
    timestamp = time.strftime("%Y-%m-%d %H:%M:%S")

    json_data = {
        "total_pages_processed": len(results),
        "timestamp": timestamp,
        "successful_translations": len([r for r in results if not r['translated_text'].startswith('[')]),
        "version": "V3 - Chunking + Table + TOC Processing",
        "pages": results
    }

    json_file = f"output/{base_name}.json"
    try:
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        print(f"âœ… JSON saved: {json_file}")
    except Exception as e:
        print(f"âŒ JSON save error: {e}")

    csv_file = f"output/{base_name}.csv"
    try:
        with open(csv_file, 'w', newline='', encoding='utf-8') as csvfile:
            fieldnames = ['Page', 'Original_Sample', 'Translation', 'Original_Length', 'Translation_Length', 'Status']
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

            writer.writeheader()
            for result in results:
                status = "Success" if not result['translated_text'].startswith('[') else "Processed"
                writer.writerow({
                    'Page': result['page_number'],
                    'Original_Sample': result['original_text'][:200] + '...' if len(result['original_text']) > 200 else result['original_text'],
                    'Translation': result['translated_text'][:200] + '...' if len(result['translated_text']) > 200 else result['translated_text'],
                    'Original_Length': result['original_char_count'],
                    'Translation_Length': result['translated_char_count'],
                    'Status': status
                })
        print(f"âœ… CSV saved: {csv_file}")
    except Exception as e:
        print(f"âŒ CSV save error: {e}")

    # Excel output (ì½ê¸° ì¢‹ì€ ë²„ì „)
    if OPENPYXL_AVAILABLE:
        excel_file = f"output/{base_name}_readable.xlsx"
        try:
            # êµ¬ì¡°í™” ì˜µì…˜ í™•ì¸
            use_structure = '--structure' in sys.argv or '--toc' in sys.argv
            if use_structure and TOC_PARSER_AVAILABLE:
                print("   ğŸ“Š ëª©ì°¨ êµ¬ì¡° ê¸°ë°˜ ì •ë¦¬ í™œì„±í™”")
                create_structured_excel(json_data, excel_file)
            else:
                create_readable_excel(json_data, excel_file)
        except Exception as e:
            print(f"âŒ Excel save error: {e}")
    else:
        print("âš ï¸  Excel output skipped (openpyxl not installed)")

def create_readable_excel(data: dict, output_path: str):
    """ì½ê¸° ì¢‹ì€ Excel íŒŒì¼ ìƒì„± (ì¤„ë°”ê¿ˆ, í•œì ì™„ë²½ í‘œì‹œ)"""
    wb = openpyxl.Workbook()

    # ì‹œíŠ¸ 1: í†µê³„
    ws_stats = wb.active
    ws_stats.title = "ğŸ“Š í†µê³„"

    # ì œëª©
    ws_stats['A1'] = "ğŸ“Š ë²ˆì—­ ê²°ê³¼ í†µê³„ (V3)"
    ws_stats['A1'].font = Font(bold=True, size=16)
    ws_stats.merge_cells('A1:B1')

    # í†µê³„ ë°ì´í„°
    stats = [
        ("", ""),
        ("ì´ í˜ì´ì§€ ìˆ˜", data['total_pages_processed']),
        ("ì„±ê³µí•œ ë²ˆì—­", data['successful_translations']),
        ("ì‹¤íŒ¨/ì²˜ë¦¬", data['total_pages_processed'] - data['successful_translations']),
        ("ì„±ê³µë¥ ", f"{data['successful_translations']/data['total_pages_processed']*100:.1f}%"),
        ("ì²˜ë¦¬ ì‹œê°„", data.get('timestamp', 'N/A')),
        ("ë²„ì „", data.get('version', 'V3')),
    ]

    row = 2
    for label, value in stats:
        ws_stats[f'A{row}'] = label
        ws_stats[f'B{row}'] = value

        if label:
            ws_stats[f'A{row}'].font = Font(bold=True)
            ws_stats[f'A{row}'].fill = PatternFill(start_color="E7E6E6", end_color="E7E6E6", fill_type="solid")
        row += 1

    ws_stats.column_dimensions['A'].width = 20
    ws_stats.column_dimensions['B'].width = 40

    # ì‹œíŠ¸ 2: ì „ì²´ ë²ˆì—­ ê²°ê³¼
    ws_all = wb.create_sheet("ğŸ“„ ì „ì²´ í˜ì´ì§€")
    create_translation_sheet(ws_all, data['pages'])

    # ì‹œíŠ¸ 3: ì„±ê³µë§Œ
    successful = [p for p in data['pages'] if not p['translated_text'].startswith('[')]
    if successful:
        ws_success = wb.create_sheet("âœ… ì„±ê³µ")
        create_translation_sheet(ws_success, successful)

    # ì‹œíŠ¸ 4: ì‹¤íŒ¨/ì²˜ë¦¬ í•„ìš”
    failed = [p for p in data['pages'] if p['translated_text'].startswith('[')]
    if failed:
        ws_failed = wb.create_sheet("âš ï¸ ì‹¤íŒ¨")
        create_translation_sheet(ws_failed, failed)

    wb.save(output_path)
    print(f"âœ… Excel saved (ì½ê¸° ì¢‹ì€ ë²„ì „): {output_path}")

def create_translation_sheet(ws, pages):
    """ë²ˆì—­ ê²°ê³¼ ì‹œíŠ¸ ìƒì„± (ì¤„ë°”ê¿ˆ ìë™ í‘œì‹œ)"""
    # í—¤ë”
    headers = ['í˜ì´ì§€', 'ì›ë¬¸', 'ë²ˆì—­ë¬¸', 'ì›ë¬¸ ê¸¸ì´', 'ë²ˆì—­ë¬¸ ê¸¸ì´', 'ìƒíƒœ', 'ì‹œê°„(ì´ˆ)']

    # í—¤ë” ìŠ¤íƒ€ì¼
    header_fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
    header_font = Font(bold=True, color="FFFFFF", size=11)

    for col, header in enumerate(headers, 1):
        cell = ws.cell(row=1, column=col)
        cell.value = header
        cell.font = header_font
        cell.fill = header_fill
        cell.alignment = Alignment(horizontal="center", vertical="center")

    # í…Œë‘ë¦¬ ìŠ¤íƒ€ì¼
    thin_border = Border(
        left=Side(style='thin'),
        right=Side(style='thin'),
        top=Side(style='thin'),
        bottom=Side(style='thin')
    )

    # ë°ì´í„° ì‘ì„±
    for row_idx, page in enumerate(pages, 2):
        is_success = not page['translated_text'].startswith('[')
        status = "âœ… ì„±ê³µ" if is_success else "âš ï¸ ì‹¤íŒ¨"

        # ì…€ ê°’ ì„¤ì •
        ws.cell(row=row_idx, column=1, value=page['page_number'])
        ws.cell(row=row_idx, column=2, value=page['original_text'][:500])  # ì›ë¬¸
        ws.cell(row=row_idx, column=3, value=page['translated_text'][:500])  # ë²ˆì—­ë¬¸
        ws.cell(row=row_idx, column=4, value=page['original_char_count'])
        ws.cell(row=row_idx, column=5, value=page['translated_char_count'])
        ws.cell(row=row_idx, column=6, value=status)
        ws.cell(row=row_idx, column=7, value=page.get('translation_time', 'N/A'))

        # ìŠ¤íƒ€ì¼ ì ìš©
        for col in range(1, 8):
            cell = ws.cell(row=row_idx, column=col)
            cell.border = thin_border

            # â­ í…ìŠ¤íŠ¸ ë˜í•‘ (ì¤„ë°”ê¿ˆ í‘œì‹œ)
            if col in [2, 3]:
                cell.alignment = Alignment(wrap_text=True, vertical="top")
            else:
                cell.alignment = Alignment(horizontal="center", vertical="center")

        # ì„±ê³µ/ì‹¤íŒ¨ ìƒ‰ìƒ
        status_cell = ws.cell(row=row_idx, column=6)
        if is_success:
            status_cell.fill = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")
        else:
            status_cell.fill = PatternFill(start_color="FFC7CE", end_color="FFC7CE", fill_type="solid")

    # ì—´ ë„ˆë¹„ ì¡°ì •
    ws.column_dimensions['A'].width = 8
    ws.column_dimensions['B'].width = 60   # ì›ë¬¸ (ë„“ê²Œ)
    ws.column_dimensions['C'].width = 60   # ë²ˆì—­ë¬¸ (ë„“ê²Œ)
    ws.column_dimensions['D'].width = 12
    ws.column_dimensions['E'].width = 12
    ws.column_dimensions['F'].width = 12
    ws.column_dimensions['G'].width = 10

    # í–‰ ë†’ì´ (ì¤„ë°”ê¿ˆ ê³ ë ¤)
    for row in range(2, len(pages) + 2):
        ws.row_dimensions[row].height = 100  # ì¶©ë¶„í•œ ë†’ì´

def create_structured_excel(data: dict, output_path: str):
    """ëª©ì°¨ êµ¬ì¡° ê¸°ë°˜ Excel íŒŒì¼ ìƒì„±"""
    parser = TOCStructureParser()
    wb = openpyxl.Workbook()

    # ëª©ì°¨ ì¶”ì¶œ (TOC í˜ì´ì§€ ì°¾ê¸°)
    toc_pages = [p for p in data['pages'] if p['page_number'] in [2, 3]]  # ì¼ë°˜ì ìœ¼ë¡œ 2-3í˜ì´ì§€
    toc_text = '\n'.join([p['original_text'] for p in toc_pages])

    # ëª©ì°¨ íŒŒì‹±
    toc_items = parser.parse_toc_text(toc_text)
    print(f"   ğŸ“– ëª©ì°¨ í•­ëª© {len(toc_items)}ê°œ ê°ì§€")

    # í˜ì´ì§€-ì„¹ì…˜ ë§¤í•‘
    page_to_section = parser.map_pages_to_sections(data['pages'])

    # ì‹œíŠ¸ 1: í†µê³„
    ws_stats = wb.active
    ws_stats.title = "ğŸ“Š í†µê³„"
    ws_stats['A1'] = "ğŸ“Š ë²ˆì—­ ê²°ê³¼ í†µê³„ (êµ¬ì¡°í™” V3)"
    ws_stats['A1'].font = Font(bold=True, size=16)
    ws_stats.merge_cells('A1:B1')

    stats = [
        ("", ""),
        ("ì´ í˜ì´ì§€ ìˆ˜", data['total_pages_processed']),
        ("ì„±ê³µí•œ ë²ˆì—­", data['successful_translations']),
        ("ëª©ì°¨ í•­ëª© ìˆ˜", len(toc_items)),
        ("ì„±ê³µë¥ ", f"{data['successful_translations']/data['total_pages_processed']*100:.1f}%"),
        ("ë²„ì „", "V3 - Chunking + TOC/í‘œ ì²˜ë¦¬"),
    ]

    row = 2
    for label, value in stats:
        ws_stats[f'A{row}'] = label
        ws_stats[f'B{row}'] = value
        if label:
            ws_stats[f'A{row}'].font = Font(bold=True)
            ws_stats[f'A{row}'].fill = PatternFill(start_color="E7E6E6", end_color="E7E6E6", fill_type="solid")
        row += 1

    ws_stats.column_dimensions['A'].width = 20
    ws_stats.column_dimensions['B'].width = 40

    # ì‹œíŠ¸ 2: êµ¬ì¡°í™”ëœ ë²ˆì—­ ê²°ê³¼
    ws_structured = wb.create_sheet("ğŸ“š êµ¬ì¡°í™”ëœ ë²ˆì—­")
    create_structured_translation_sheet(ws_structured, data['pages'], page_to_section, parser)

    # ì‹œíŠ¸ 3: ëª©ì°¨
    ws_toc = wb.create_sheet("ğŸ“‘ ëª©ì°¨")
    create_toc_sheet(ws_toc, toc_items)

    wb.save(output_path)
    print(f"âœ… Excel saved (êµ¬ì¡°í™” ë²„ì „): {output_path}")

def create_structured_translation_sheet(ws, pages, page_to_section: Dict, parser: TOCStructureParser):
    """êµ¬ì¡°í™”ëœ ë²ˆì—­ ì‹œíŠ¸"""
    headers = ['ì„¹ì…˜', 'í˜ì´ì§€', 'ì›ë¬¸', 'ë²ˆì—­ë¬¸', 'ìƒíƒœ']

    # í—¤ë” ìŠ¤íƒ€ì¼
    header_fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
    header_font = Font(bold=True, color="FFFFFF", size=11)

    for col, header in enumerate(headers, 1):
        cell = ws.cell(row=1, column=col)
        cell.value = header
        cell.font = header_font
        cell.fill = header_fill
        cell.alignment = Alignment(horizontal="center", vertical="center")

    # í…Œë‘ë¦¬
    thin_border = Border(
        left=Side(style='thin'),
        right=Side(style='thin'),
        top=Side(style='thin'),
        bottom=Side(style='thin')
    )

    # ì„¹ì…˜ë³„ë¡œ ê·¸ë£¹í™”
    current_section = None
    row_idx = 2

    for page in sorted(pages, key=lambda x: x['page_number']):
        page_num = page['page_number']
        section_num = page_to_section.get(page_num, "")

        # ìƒˆ ì„¹ì…˜ ì‹œì‘ ì‹œ í—¤ë” ì¶”ê°€
        if section_num and section_num != current_section:
            current_section = section_num
            section_info = parser.get_section_info(section_num)

            # ì„¹ì…˜ í—¤ë” í–‰
            level = section_num.count('.') + 1
            indent = "  " * (level - 1)
            section_title = f"{indent}{section_num}"
            if section_info:
                section_title += f" {section_info.title}"

            ws.cell(row=row_idx, column=1, value=section_title)
            ws.merge_cells(f'A{row_idx}:E{row_idx}')

            header_cell = ws.cell(row=row_idx, column=1)
            if level == 1:
                header_cell.fill = PatternFill(start_color="BDD7EE", end_color="BDD7EE", fill_type="solid")
                header_cell.font = Font(bold=True, size=12)
            elif level == 2:
                header_cell.fill = PatternFill(start_color="DDEBF7", end_color="DDEBF7", fill_type="solid")
                header_cell.font = Font(bold=True, size=11)
            else:
                header_cell.fill = PatternFill(start_color="E7E6E6", end_color="E7E6E6", fill_type="solid")
                header_cell.font = Font(bold=True, size=10)

            row_idx += 1

        # í˜ì´ì§€ ë°ì´í„°
        is_success = not page['translated_text'].startswith('[')
        status = "âœ…" if is_success else "âš ï¸"

        ws.cell(row=row_idx, column=1, value=section_num)
        ws.cell(row=row_idx, column=2, value=page_num)
        ws.cell(row=row_idx, column=3, value=page['original_text'][:300])
        ws.cell(row=row_idx, column=4, value=page['translated_text'][:300])
        ws.cell(row=row_idx, column=5, value=status)

        # ìŠ¤íƒ€ì¼
        for col in range(1, 6):
            cell = ws.cell(row=row_idx, column=col)
            cell.border = thin_border
            if col in [3, 4]:
                cell.alignment = Alignment(wrap_text=True, vertical="top")
            else:
                cell.alignment = Alignment(horizontal="center", vertical="center")

        if is_success:
            ws.cell(row=row_idx, column=5).fill = PatternFill(start_color="C6EFCE", end_color="C6EFCE", fill_type="solid")

        ws.row_dimensions[row_idx].height = 60
        row_idx += 1

    # ì—´ ë„ˆë¹„
    ws.column_dimensions['A'].width = 15
    ws.column_dimensions['B'].width = 8
    ws.column_dimensions['C'].width = 50
    ws.column_dimensions['D'].width = 50
    ws.column_dimensions['E'].width = 8

def create_toc_sheet(ws, toc_items: List[TOCItem]):
    """ëª©ì°¨ ì‹œíŠ¸"""
    headers = ['ë²ˆí˜¸', 'ì œëª©', 'ë ˆë²¨', 'í˜ì´ì§€']

    # í—¤ë”
    header_fill = PatternFill(start_color="4472C4", end_color="4472C4", fill_type="solid")
    header_font = Font(bold=True, color="FFFFFF", size=11)

    for col, header in enumerate(headers, 1):
        cell = ws.cell(row=1, column=col)
        cell.value = header
        cell.font = header_font
        cell.fill = header_fill
        cell.alignment = Alignment(horizontal="center", vertical="center")

    # ëª©ì°¨ í•­ëª©
    for row_idx, item in enumerate(toc_items, 2):
        indent = "  " * (item.level - 1)

        ws.cell(row=row_idx, column=1, value=item.number)
        ws.cell(row=row_idx, column=2, value=f"{indent}{item.title}")
        ws.cell(row=row_idx, column=3, value=item.level)
        ws.cell(row=row_idx, column=4, value=item.page)

        # ë ˆë²¨ë³„ ìƒ‰ìƒ
        if item.level == 1:
            fill = PatternFill(start_color="BDD7EE", end_color="BDD7EE", fill_type="solid")
        elif item.level == 2:
            fill = PatternFill(start_color="DDEBF7", end_color="DDEBF7", fill_type="solid")
        else:
            fill = PatternFill(start_color="F2F2F2", end_color="F2F2F2", fill_type="solid")

        ws.cell(row=row_idx, column=1).fill = fill
        ws.cell(row=row_idx, column=2).fill = fill

    # ì—´ ë„ˆë¹„
    ws.column_dimensions['A'].width = 12
    ws.column_dimensions['B'].width = 50
    ws.column_dimensions['C'].width = 8
    ws.column_dimensions['D'].width = 10

def main():
    """Main translation workflow - V3 with chunking and table processing"""

    pdf_file = "input/XY-A ATSå¼€å‘å¯¹IBCéœ€æ±‚ë¬¸ì„œ_V0.0.pdf"

    # ëª…ë ¹ì¤„ ì˜µì…˜ í™•ì¸
    use_structure = '--structure' in sys.argv or '--toc' in sys.argv

    # ì „ì²´ ë²ˆì—­ ë˜ëŠ” íŠ¹ì • í˜ì´ì§€ë§Œ ë²ˆì—­
    if '--all' in sys.argv:
        target_pages = None  # ì „ì²´ í˜ì´ì§€
        mode_text = "ì „ì²´ 70í˜ì´ì§€"
    else:
        target_pages = [2, 3]  # TOC í˜ì´ì§€ë§Œ (í…ŒìŠ¤íŠ¸ìš©)
        mode_text = f"í˜ì´ì§€ {target_pages}"

    print("ğŸš€ ê°œì„ ëœ PDF ë²ˆì—­ê¸° V3 - 2ë‹¨ê³„ ê°œì„ ì‚¬í•­ ì ìš©")
    print("="*80)
    print("âœ¨ V3 ì‹ ê·œ ê¸°ëŠ¥:")
    print("   1. ìŠ¤ë§ˆíŠ¸ ì²­í¬ ë¶„í•  (800ì ë‹¨ìœ„, 100ì ì˜¤ë²„ë©)")
    print("   2. í‘œ/í…Œì´ë¸” ìë™ ê°ì§€ ë° ì „ìš© ì²˜ë¦¬")
    print("   3. TOC êµ¬ì¡° íŒŒì‹± ë° í•­ëª©ë³„ ë²ˆì—­")
    if use_structure:
        print("   ğŸ“Š ëª©ì°¨ êµ¬ì¡° ê¸°ë°˜ ì •ë¦¬: í™œì„±í™”")
    print("="*80)
    print(f"\nğŸ¯ ëŒ€ìƒ: {mode_text}")
    if target_pages is None:
        print(f"â±ï¸  ì˜ˆìƒ ì‹œê°„: ì•½ 5-7ë¶„")

    if not os.path.exists(pdf_file):
        print(f"âŒ PDF not found: {pdf_file}")
        return

    # PDF ì¶”ì¶œ
    pages_text = extract_text_from_pdf(pdf_file, max_pages=None)
    if not pages_text:
        print("âŒ No text extracted")
        return

    # íƒ€ê²Ÿ í˜ì´ì§€ë§Œ í•„í„°ë§
    if target_pages:
        pages_to_translate = [(num, text) for num, text in pages_text if num in target_pages]
    else:
        pages_to_translate = pages_text

    results = []
    total = len(pages_to_translate)

    for i, (page_num, text) in enumerate(pages_to_translate, 1):
        print(f"\n{'='*80}")
        print(f"ğŸ“‹ Processing Page {page_num} ({i}/{total})...")

        sample = safe_text_cleaning(text)[:100]
        print(f"   ğŸ“ Sample: {sample}...")

        print("   ğŸ”„ V3 ë²ˆì—­ ì¤‘...")
        start_time = time.time()
        translated = translate_with_google_robust(text, use_chunking=True)
        translation_time = time.time() - start_time

        result_sample = translated[:100]
        print(f"   âœ… Result: {result_sample}...")
        print(f"   â±ï¸  Time: {translation_time:.1f}s")

        results.append({
            "page_number": page_num,
            "original_text": text,
            "translated_text": translated,
            "original_char_count": len(text),
            "translated_char_count": len(translated),
            "translation_time": round(translation_time, 2)
        })

        if i < total:
            time.sleep(2)

    # ê²°ê³¼ ì €ì¥
    print(f"\n{'='*80}")
    print("ğŸ’¾ Saving results...")
    create_outputs(results, "improved_translation_v3_results")

    # í†µê³„
    successful = len([r for r in results if not r['translated_text'].startswith('[')])
    print(f"\nğŸ“Š Summary:")
    print(f"   Pages processed: {len(results)}")
    print(f"   Successful: {successful} ({successful/len(results)*100:.1f}%)")

    print("\nâœ… V3 Translation completed!")

if __name__ == "__main__":
    main()
