#!/usr/bin/env python3
"""
ë²ˆì—­ ì‹¤íŒ¨ ì›ì¸ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
"""

import json
import pandas as pd
from collections import defaultdict, Counter
import re

def load_translation_data(json_path):
    """JSON íŒŒì¼ì—ì„œ ë²ˆì—­ ë°ì´í„° ë¡œë“œ"""
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    return data

def analyze_failure_patterns(data):
    """ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„"""

    failed_pages = []
    failure_types = defaultdict(list)

    for page in data['pages']:
        translated = page['translated_text']

        # ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ê°ì§€
        is_failed = False
        failure_reason = None

        if '[Translation failed' in translated:
            is_failed = True
            # ì‹¤íŒ¨ ë©”ì‹œì§€ ì¶”ì¶œ
            match = re.search(r'\[Translation failed.*?\]', translated)
            if match:
                failure_reason = match.group(0)

        elif '[TOC -' in translated:
            is_failed = True
            failure_reason = '[TOC Translation Failed]'

        if is_failed:
            failed_pages.append({
                'page_number': page['page_number'],
                'original_length': page['original_char_count'],
                'failure_message': failure_reason,
                'original_preview': page['original_text'][:200],
                'translation_time': page['translation_time']
            })

            # ì‹¤íŒ¨ ìœ í˜• ë¶„ë¥˜
            if 'after 2 attempts' in translated:
                failure_types['ì¬ì‹œë„ í›„ ì‹¤íŒ¨'].append(page['page_number'])
            elif 'sequence item' in translated or 'NoneType' in translated:
                failure_types['íƒ€ì… ì—ëŸ¬'].append(page['page_number'])
            elif 'TOC' in translated:
                failure_types['ëª©ì°¨(TOC) ì²˜ë¦¬ ì‹¤íŒ¨'].append(page['page_number'])
            else:
                failure_types['ê¸°íƒ€'].append(page['page_number'])

    return failed_pages, failure_types

def categorize_content_types(failed_pages):
    """ì‹¤íŒ¨í•œ í˜ì´ì§€ì˜ ì½˜í…ì¸  ìœ í˜• ë¶„ë¥˜"""

    content_patterns = {
        'ëª©ì°¨/ì¸ë±ìŠ¤': [r'ç›®\s*å½•', r'åºå·.*ç‰ˆæœ¬.*å˜æ›´å†…å®¹', r'^\s*-\s*\d+\s*-'],
        'í‘œ/í…Œì´ë¸”': [r'è¡¨\d+', r'â”ƒ', r'â”‚', r'å·¥å†µ', r'å‰ææ¡ä»¶.*æµ‹è¯•å·¥å†µ.*ç›®æ ‡è¦æ±‚'],
        'ê¸°ìˆ  ì‚¬ì–‘': [r'ATSæ¨¡å¼', r'IBC', r'åŠŸèƒ½è¦æ±‚', r'çŠ¶æ€', r'TBD', r'æ¡ä»¶'],
        'ê¸´ ë¦¬ìŠ¤íŠ¸': [r'^\d+\..*\n\d+\..*\n\d+\.', r'[a-z]\.\s.*\n[a-z]\.\s'],
    }

    categorized = defaultdict(list)

    for page in failed_pages:
        original = page['original_preview']
        page_num = page['page_number']

        matched = False
        for category, patterns in content_patterns.items():
            for pattern in patterns:
                if re.search(pattern, original):
                    categorized[category].append(page_num)
                    matched = True
                    break
            if matched:
                break

        if not matched:
            categorized['ì¼ë°˜ í…ìŠ¤íŠ¸'].append(page_num)

    return categorized

def analyze_length_correlation(failed_pages):
    """ì‹¤íŒ¨ì™€ ì›ë¬¸ ê¸¸ì´ì˜ ìƒê´€ê´€ê³„ ë¶„ì„"""

    lengths = [p['original_length'] for p in failed_pages]

    length_analysis = {
        'í‰ê·  ê¸¸ì´': sum(lengths) / len(lengths) if lengths else 0,
        'ìµœì†Œ ê¸¸ì´': min(lengths) if lengths else 0,
        'ìµœëŒ€ ê¸¸ì´': max(lengths) if lengths else 0,
        'ì¤‘ì•™ê°’': sorted(lengths)[len(lengths)//2] if lengths else 0,
    }

    # ê¸¸ì´ë³„ ë¶„í¬
    length_distribution = {
        'ë§¤ìš° ì§§ìŒ (<500ì)': sum(1 for l in lengths if l < 500),
        'ì§§ìŒ (500-1000ì)': sum(1 for l in lengths if 500 <= l < 1000),
        'ì¤‘ê°„ (1000-2000ì)': sum(1 for l in lengths if 1000 <= l < 2000),
        'ê¸´í¸ (2000-3000ì)': sum(1 for l in lengths if 2000 <= l < 3000),
        'ë§¤ìš° ê¸º (3000ì+)': sum(1 for l in lengths if l >= 3000),
    }

    return length_analysis, length_distribution

def generate_analysis_report(data):
    """ì¢…í•© ë¶„ì„ ë³´ê³ ì„œ ìƒì„±"""

    print("="*80)
    print("ğŸ“Š ë²ˆì—­ ì‹¤íŒ¨ ì›ì¸ ë¶„ì„ ë³´ê³ ì„œ")
    print("="*80)

    # 1. ê¸°ë³¸ í†µê³„
    total_pages = data['total_pages_processed']
    success_pages = data['successful_translations']
    failed_count = total_pages - success_pages

    print(f"\nğŸ“ˆ ê¸°ë³¸ í†µê³„")
    print(f"  - ì´ í˜ì´ì§€: {total_pages}")
    print(f"  - ì„±ê³µ: {success_pages} ({success_pages/total_pages*100:.1f}%)")
    print(f"  - ì‹¤íŒ¨: {failed_count} ({failed_count/total_pages*100:.1f}%)")

    # 2. ì‹¤íŒ¨ íŒ¨í„´ ë¶„ì„
    failed_pages, failure_types = analyze_failure_patterns(data)

    print(f"\nğŸ” ì‹¤íŒ¨ ìœ í˜• ë¶„ë¥˜")
    for failure_type, pages in sorted(failure_types.items(), key=lambda x: len(x[1]), reverse=True):
        print(f"  - {failure_type}: {len(pages)}ê±´")
        print(f"    í˜ì´ì§€: {', '.join(map(str, sorted(pages)[:10]))}" +
              (f" ... ì™¸ {len(pages)-10}ê±´" if len(pages) > 10 else ""))

    # 3. ì½˜í…ì¸  ìœ í˜• ë¶„ì„
    content_categories = categorize_content_types(failed_pages)

    print(f"\nğŸ“„ ì‹¤íŒ¨í•œ ì½˜í…ì¸  ìœ í˜•")
    for content_type, pages in sorted(content_categories.items(), key=lambda x: len(x[1]), reverse=True):
        print(f"  - {content_type}: {len(pages)}ê±´")
        print(f"    í˜ì´ì§€: {', '.join(map(str, sorted(pages)[:10]))}" +
              (f" ... ì™¸ {len(pages)-10}ê±´" if len(pages) > 10 else ""))

    # 4. ê¸¸ì´ ìƒê´€ê´€ê³„ ë¶„ì„
    length_analysis, length_distribution = analyze_length_correlation(failed_pages)

    print(f"\nğŸ“ ì‹¤íŒ¨ í˜ì´ì§€ ê¸¸ì´ ë¶„ì„")
    for metric, value in length_analysis.items():
        print(f"  - {metric}: {value:.0f}ì")

    print(f"\nğŸ“Š ê¸¸ì´ë³„ ì‹¤íŒ¨ ë¶„í¬")
    for range_name, count in length_distribution.items():
        percentage = (count / len(failed_pages) * 100) if failed_pages else 0
        bar = "â–ˆ" * int(percentage / 5)
        print(f"  - {range_name:20s}: {count:3d}ê±´ ({percentage:5.1f}%) {bar}")

    # 5. ìƒì„¸ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ìƒ˜í”Œ
    print(f"\nğŸ”¬ ìƒì„¸ ì‹¤íŒ¨ ì¼€ì´ìŠ¤ (ìƒ˜í”Œ 5ê±´)")
    for i, page in enumerate(failed_pages[:5], 1):
        print(f"\n  [{i}] í˜ì´ì§€ {page['page_number']}")
        print(f"      ì›ë¬¸ ê¸¸ì´: {page['original_length']}ì")
        print(f"      ì²˜ë¦¬ ì‹œê°„: {page['translation_time']:.2f}ì´ˆ")
        print(f"      ì‹¤íŒ¨ ë©”ì‹œì§€: {page['failure_message']}")
        print(f"      ì›ë¬¸ ë¯¸ë¦¬ë³´ê¸°: {page['original_preview'][:150]}...")

    # 6. ì£¼ìš” ì‹¤íŒ¨ ì›ì¸ ìš”ì•½
    print(f"\nğŸ’¡ ì£¼ìš” ì‹¤íŒ¨ ì›ì¸ ë¶„ì„")

    # ì›ì¸ë³„ ë¶„ì„
    reasons = []

    if 'ëª©ì°¨(TOC) ì²˜ë¦¬ ì‹¤íŒ¨' in failure_types and len(failure_types['ëª©ì°¨(TOC) ì²˜ë¦¬ ì‹¤íŒ¨']) > 0:
        reasons.append({
            'reason': 'ëª©ì°¨(TOC) êµ¬ì¡° ì²˜ë¦¬ ì˜¤ë¥˜',
            'description': 'ë³µì¡í•œ ëª©ì°¨ êµ¬ì¡°ì—ì„œ NoneType ì—ëŸ¬ ë°œìƒ',
            'affected': len(failure_types['ëª©ì°¨(TOC) ì²˜ë¦¬ ì‹¤íŒ¨']),
            'solution': 'ëª©ì°¨ ì²˜ë¦¬ ë¡œì§ ê°œì„  í•„ìš” (None ê°’ í•¸ë“¤ë§)'
        })

    if 'ì¬ì‹œë„ í›„ ì‹¤íŒ¨' in failure_types and len(failure_types['ì¬ì‹œë„ í›„ ì‹¤íŒ¨']) > 0:
        reasons.append({
            'reason': 'ë²ˆì—­ API 2íšŒ ì‹œë„ í›„ ì‹¤íŒ¨',
            'description': 'API í˜¸ì¶œì´ 2ë²ˆ ëª¨ë‘ ì‹¤íŒ¨í•˜ì—¬ ë²ˆì—­ ë¶ˆê°€',
            'affected': len(failure_types['ì¬ì‹œë„ í›„ ì‹¤íŒ¨']),
            'solution': 'ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€, ì²­í¬ í¬ê¸° ì¡°ì •, íƒ€ì„ì•„ì›ƒ ì¦ê°€'
        })

    if 'í‘œ/í…Œì´ë¸”' in content_categories and len(content_categories['í‘œ/í…Œì´ë¸”']) > 5:
        reasons.append({
            'reason': 'ë³µì¡í•œ í‘œ/í…Œì´ë¸” êµ¬ì¡°',
            'description': 'í‘œ í˜•íƒœì˜ ë°ì´í„°ê°€ ë²ˆì—­ ì¤‘ êµ¬ì¡° ì†ì‹¤',
            'affected': len(content_categories['í‘œ/í…Œì´ë¸”']),
            'solution': 'í‘œ ì „ìš© ì²˜ë¦¬ ë¡œì§ ì¶”ê°€, êµ¬ì¡° ë³´ì¡´ ì•Œê³ ë¦¬ì¦˜ ì ìš©'
        })

    # ê¸´ í…ìŠ¤íŠ¸ ë¶„ì„
    long_texts = [p for p in failed_pages if p['original_length'] > 1000]
    if len(long_texts) > 10:
        reasons.append({
            'reason': 'ê¸´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ì‹¤íŒ¨',
            'description': f'1000ì ì´ìƒì˜ ê¸´ í…ìŠ¤íŠ¸ê°€ {len(long_texts)}ê±´ ì‹¤íŒ¨',
            'affected': len(long_texts),
            'solution': 'ì²­í¬ ë¶„í•  ì „ëµ ê°œì„ , ì»¨í…ìŠ¤íŠ¸ ìœ ì§€ ë°©ë²• ê°œì„ '
        })

    for i, reason_info in enumerate(reasons, 1):
        print(f"\n  {i}. {reason_info['reason']}")
        print(f"     ì„¤ëª…: {reason_info['description']}")
        print(f"     ì˜í–¥: {reason_info['affected']}ê±´")
        print(f"     í•´ê²°ë°©ì•ˆ: {reason_info['solution']}")

    # 7. ê¶Œì¥ì‚¬í•­
    print(f"\nâœ… ê°œì„  ê¶Œì¥ì‚¬í•­")
    print(f"  1. ëª©ì°¨/TOC ì²˜ë¦¬ ë¡œì§ì—ì„œ None ê°’ ì²´í¬ ê°•í™”")
    print(f"  2. ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ ê°œì„  (ì§€ìˆ˜ ë°±ì˜¤í”„, ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€)")
    print(f"  3. ë³µì¡í•œ í‘œ êµ¬ì¡° ì „ìš© ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì¶”ê°€")
    print(f"  4. ê¸´ í…ìŠ¤íŠ¸ëŠ” ë” ì‘ì€ ì²­í¬ë¡œ ë¶„í• ")
    print(f"  5. API íƒ€ì„ì•„ì›ƒ ì„¤ì • ì¦ê°€")
    print(f"  6. ì‹¤íŒ¨ ì‹œ í´ë°±(fallback) ë²ˆì—­ ì„œë¹„ìŠ¤ ì‚¬ìš©")

    print("\n" + "="*80)

    return failed_pages, failure_types, content_categories

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""

    json_path = '/Users/kimjinmyung/Desktop/GIT/TranslatewithRag/final_translation_results.json'

    data = load_translation_data(json_path)
    failed_pages, failure_types, content_categories = generate_analysis_report(data)

    # ì¶”ê°€ CSV ì¶œë ¥ (ì‹¤íŒ¨ ìƒì„¸ ë‚´ì—­)
    if failed_pages:
        df_failures = pd.DataFrame(failed_pages)
        csv_path = '/Users/kimjinmyung/Desktop/GIT/TranslatewithRag/ì‹¤íŒ¨_ìƒì„¸ë¶„ì„.csv'
        df_failures.to_csv(csv_path, index=False, encoding='utf-8-sig')
        print(f"\nğŸ“ ìƒì„¸ ì‹¤íŒ¨ ë¶„ì„ CSV ì €ì¥: {csv_path}")

if __name__ == '__main__':
    main()
